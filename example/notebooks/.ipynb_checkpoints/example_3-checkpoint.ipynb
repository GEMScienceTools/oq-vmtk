{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d4efaf0-0346-4d8b-a624-c57442492207",
   "metadata": {},
   "source": [
    "# Example 3: Post-Processing and Visualising Nonlinear Time-History Analysis Results using the \"postprocessor\" and \"plotter\" Classes\n",
    "\n",
    "## Introduction\n",
    "\n",
    "\r\n",
    "\r\n",
    "This Jupyter notebook is designed to postprocess and visualize the results of a **Nonlinear Time History AnalysiLs (NTHA)** conducted on a **Multi-Degree of Freedom (MDOF)** structural sy in Example_2st\n",
    "Post-processing cloud analysis \n",
    "\n",
    "The key objective of this notebook is:\n",
    "\n",
    "**Result Post-Processing**: Extract and visualize critical response metrics to:\n",
    "   - Visualise seismic demands such as peak storey drifts (PSD) and peak floor accelerations (PFA) along the building height \n",
    "   - Perform and visualise cloud analyses to characterise the engineering demand parameter (i.e., maximum peak storey drift) given intensity measure levels distribution (or EDP=MPSD|IM)\n",
    "   - Estimate the median seismic intensities and total associated dispersions (i.e., accounting for record-to-record variability and modelling uncertainty) corresponding to user-defined demand-based damage thresholds \n",
    "   - Calculate and visualise damage probabilities (i.e., fragility functions) corresponding to distinct structural damage states\n",
    "\n",
    "The notebook provides a step-by-step guide, covering each phase from MDOF model calibration, setup to input parameter configuration, analysis execution, and detailed results interpretation. Users should have some familiarity with python scripts, structural dynamics and computational modeling to fully benefit from this material.\n",
    "\n",
    "---\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "---\n",
    "\n",
    "## Workflow Overview\n",
    "\n",
    "1. **Import and preprocess the results** from a nonlinear time history analysis, which includes peak response metrics and engineering demand parameters.\n",
    "2. **Evaluate the performance** of the system by examining the IM-EDP relationship via regression analysis on cloud results.\n",
    "3. **Visualize the response** of the system in terms of cloud analyses, fragility functions and seismic demands along the height of the building.\n",
    "\n",
    "By the end of this notebook, users will have a complete, adaptable script for nonlinear dynamic analyses on MDOF structures, supporting a range of investigation scenarios and performance assessments.\n",
    "\n",
    "Letâ€™s begin by defining initial input parameters and loading the required libraries and setting up the initial parameters for the MDOF model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "174c0a74-f28f-467d-9b48-3e24d0591f06",
   "metadata": {},
   "source": [
    "# Required Input Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99d02025-e25e-45f3-811f-f01abf4d4ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intensity measures to use for cloud analyses\n",
    "IMTs      = ['PGA', 'SA(0.3s)', 'SA(0.6s)', 'SA(1.0s)']\n",
    "# Damage thresholds (maximum peak storey drift values)\n",
    "damage_thresholds    =  [0.00150, 0.00545, 0.00952, 0.0135]\n",
    "# The lower limit to be applied for censoring edp values (below 0.1 the minimum threshold for slight damage is considered a negligible case)\n",
    "lower_limit = 0.1*damage_thresholds[0]\n",
    "# The upper limit to be applied for consoring edp values (above 1.5 the maximum threshold is considered a collapse case) \n",
    "censored_limit = 1.5*damage_thresholds[-1]    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8379d2e4-b108-4101-8efd-0868a0195731",
   "metadata": {},
   "source": [
    "# Initialize Libraries and Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f07a0fa-5b71-49a0-ab78-c97b21300e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import sys\n",
    "import os\n",
    "\n",
    "vulnerability_toolkit = os.path.abspath(os.path.join(\"..\",\"..\"))\n",
    "sys.path.append(vulnerability_toolkit)\n",
    "os.chdir(os.path.join(vulnerability_toolkit,'src'))\n",
    "\n",
    "from src.postprocessor import *\n",
    "from src.plotter import *\n",
    "from src.utilities import *\n",
    "\n",
    "start = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "229fa396-0d68-4f92-9f10-ec1cce738070",
   "metadata": {},
   "source": [
    "# Define Directories and Import Analysis Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79a6ea8f-981b-4d52-8952-d3dfc1f7448a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the directory of the ground-motion records and import the intensity measure pickle file containing all IMs (processed from example 1)\n",
    "gmDir  = f'{vulnerability_toolkit}/example/in/records'            \n",
    "ims = import_from_pkl(f'{gmDir}/ims.pkl')     \n",
    "\n",
    "# Define the main output directory and import the analysis output from a pickle file using the \"import_from_pkl\" function from \"utilities\"\n",
    "nlthaOutDir = f'{vulnerability_toolkit}/example/out/nltha' \n",
    "ansys_dict = import_from_pkl(f'{nlthaOutDir}/ansys.pkl') # processed from example 2\n",
    "edps = ansys_dict['mdof_max_peak_drift_list'] # Import the engineering demand parameters (i.e., mpsd) from the analysis dictionary (processed from example 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15cc76f7-4b3e-44b9-bbed-0e6b7c34dd4a",
   "metadata": {},
   "source": [
    "# Post-Processing: Cloud and Fragility Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93315609-c1ce-445a-8232-476470c5f6b7",
   "metadata": {},
   "source": [
    "#### The cloud analysis module (do_cloud_analysis) of the \"postprocessor\" class requires five mandatory arguments and one optional:\n",
    "1. The intensity measure levels of the ground-motion records (imls)\n",
    "2. The engineering demand parameters from the analysis (edps)\n",
    "3. The demand-based damage thresholds (damage_thresholds)\n",
    "4. The lower edp censoring limit (lower_limit)\n",
    "5. The upper edp censoring limit (censored_limit)\n",
    "6. OPTIONAL: The modelling uncertainty (sigma_build2build)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e95bb1af-551d-4db1-b3d1-c3b4e0ffedb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop over the intensity measure types and perform cloud regression to fit \n",
    "# the probabilistic seismic demand-capacity model\n",
    "for _, currentIMT in enumerate(IMTs):\n",
    "    \n",
    "    # Import the current intensity measure type\n",
    "    imls = ims[f'{currentIMT}']                   \n",
    "\n",
    "    # Initialise the postprocessor and plotter classes\n",
    "    pp = postprocessor()\n",
    "    pl = plotter()\n",
    "    \n",
    "    # Process cloud analysis results using the \"do_cloud_analysis\" function called from \"postprocessor\" \n",
    "    # The output will be automatically stored in a dictionary\n",
    "    cloud_dict = pp.do_cloud_analysis(imls,\n",
    "                                      edps,\n",
    "                                      damage_thresholds,\n",
    "                                      lower_limit,\n",
    "                                      censored_limit) \n",
    "    \n",
    "    ## Create a figures directory for each building class\n",
    "    figures_directory = f'{vulnerability_toolkit}/example/out/nltha/figures'    \n",
    "    if not os.path.exists(f'{figures_directory}'):\n",
    "        os.makedirs(f'{figures_directory}')\n",
    "    \n",
    "    ## Visualise the cloud analysis results\n",
    "    pl.plot_cloud_analysis(cloud_dict, \n",
    "                        figures_directory, \n",
    "                        plot_label = f'cloud_analysis_{currentIMT}',\n",
    "                        xlabel = f'{currentIMT} [g]', \n",
    "                        ylabel = r'Maximum Peak Storey Drift, $\\theta_{max}$ [%]')\n",
    "\n",
    "    ## Visualise the fragility functions\n",
    "    pl.plot_fragility_analysis(cloud_dict,\n",
    "                            figures_directory,\n",
    "                            plot_label = f'fragility_{currentIMT}',\n",
    "                            xlabel = f'{currentIMT}')\n",
    "\n",
    "    ## Visualise the seismic demands\n",
    "    pl.plot_demand_profiles(ansys_dict['mdof_peak_drift_list'], \n",
    "                            ansys_dict['mdof_peak_accel_list'], \n",
    "                            ansys_dict['control_nodes'], \n",
    "                            figures_directory,\n",
    "                            plot_label=\"seismic_demand_profiles\")\n",
    "        \n",
    "    ## Visualise the entire set of results using subplots\n",
    "    pl.plot_ansys_results(cloud_dict,\n",
    "                          ansys_dict['mdof_peak_drift_list'],\n",
    "                          ansys_dict['mdof_peak_accel_list'],\n",
    "                          ansys_dict['control_nodes'],\n",
    "                          figures_directory,\n",
    "                          plot_label = 'analysis_output_{currentIMT}',\n",
    "                          cloud_xlabel = f'{currentIMT}',\n",
    "                          cloud_ylabel = r'Maximum Peak Storey Drift, $\\theta_{max}$ [%]')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40313e23-e64a-4bf7-a8bb-2662a4c93c95",
   "metadata": {},
   "source": [
    "## Post-Processing: Vulnerability Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e22987-92ab-4234-8dc5-82c744cd8bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define consequence model to relate structural damage to a decision variable (i.e., expected loss ratio) \n",
    "consequence_model = [0.05, 0.20, 0.60, 1.00] # damage-to-loss ratios\n",
    "\n",
    "# To derive the vulnerability, the consequence model needs to convolved with the fragility functions. \n",
    "# To do so, we can use the \"get_vulnerability_function\" method from the \"postprocessor\" class.\n",
    "structural_loss = pp.get_vulnerability_function(cloud_dict['poes'],\n",
    "                                                consequence_model)\n",
    "\n",
    "# Plot the vulnerability function to visualise\n",
    "plt.plot(cloud_dict['intensities'], structural_loss, label = 'Structural Loss')\n",
    "plt.xlabel('Spectral Acceleration, Sa(T=1.0s) [g]', fontsize= FONTSIZE_1)\n",
    "plt.ylabel('Expected Loss Ratio', fontsize = FONTSIZE_1)\n",
    "plt.legend(loc = 'lower right')\n",
    "plt.grid(visible=True, which='major')\n",
    "plt.grid(visible=True, which='minor')\n",
    "plt.xlim([0.00, 5.00])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39c357a7-e6b1-43b3-a58b-f9cee926cdc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "end = time.time()\n",
    "print('Elapsed Time:', (end - start)/60, 'minutes')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
