{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d4efaf0-0346-4d8b-a624-c57442492207",
   "metadata": {},
   "source": [
    "# Example 3: Post-Processing and Visualising Nonlinear Time-History Analysis Results using the \"postprocessor\" and \"plotter\" Classes\n",
    "\n",
    "## Introduction\n",
    "\n",
    "\r\n",
    "This Jupyter notebook is designed to postprocess and visualize thcloud analysis e resultfollowing a a **Nonlinear Time History AnalysiLs (NTHA)** conducted on a **Multi-Degree of Freedom (MDOF)*(after runningie Example in demos folder)s \n",
    "\n",
    "Thmain goals of this notebookok is:\n",
    "\n",
    "**Result Post-Processing**: Extract and visualize critical response metrics to:\n",
    "   - Visualise seismic demands such as peak storey drifts (PSD) and peak floor accelerations (PFA) along the building height \n",
    "   - Perform and visualise cloud analyses to characterise the engineering demand parameter (i.e., maximum peak storey drift) given intensity measure levels distribution (or EDP=MPSD|IM)\n",
    "   - Estimate the median seismic intensities and total associated dispersions (i.e., accounting for record-to-record variability and modelling uncertainty) corresponding to user-defined demand-based damage thresholds \n",
    "   - Calculate and visualise damage probabilities (i.e., fragility functions) corresponding to distinct structural damage states\n",
    "\n",
    "The notebook provides a step-by-step guide, covering each phase from MDOF model calibration, setup to input parameter configuration, analysis execution, and detailed results interpretation. Users should have some familiarity with python scripts, structural dynamics and computational modeling to fully benefit from this material."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8379d2e4-b108-4101-8efd-0868a0195731",
   "metadata": {},
   "source": [
    "## Initialize Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f07a0fa-5b71-49a0-ab78-c97b21300e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "# Set paths\n",
    "vmtk = os.path.abspath(os.path.join(\"..\",\"..\"))\n",
    "sys.path.append(vmtk)\n",
    "os.chdir(os.path.join(vmtk,'src'))\n",
    "\n",
    "# Import the classes necessary for postprocessing and visualising structural analysis results\n",
    "from src.postprocessor import postprocessor\n",
    "from src.plotter       import plotter\n",
    "from src.utilities     import import_from_pkl\n",
    "\n",
    "# Initialise the postprocessor and plotter classes\n",
    "pp = postprocessor()\n",
    "pl = plotter()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79c3bb3b-66b0-4709-820a-2df044b86d51",
   "metadata": {},
   "source": [
    "## Define Plotting Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8f3d8a1-932c-41fe-bc86-4373b6d1a4e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "FONTSIZE_1 = 16\n",
    "FONTSIZE_2 = 14\n",
    "FONTSIZE_3 = 12\n",
    "\n",
    "LINEWIDTH_1= 3\n",
    "LINEWIDTH_2= 2\n",
    "LINEWIDTH_3 = 1\n",
    "\n",
    "RESOLUTION = 500\n",
    "\n",
    "MARKER_SIZE_1 = 100\n",
    "MARKER_SIZE_2 = 60\n",
    "MARKER_SIZE_3 = 10\n",
    "\n",
    "COLOR = \"#399283\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "587a2802-9b91-48d1-9c69-8a4fadc7255c",
   "metadata": {},
   "source": [
    "## Required Input Parameters ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8362a70-cac3-4440-8306-b5926b7407bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intensity measures to use for cloud analyses\n",
    "IMTs      = ['PGA', 'SA(0.3s)', 'SA(0.6s)', 'SA(1.0s)','AvgSA']\n",
    "\n",
    "# Damage thresholds (maximum peak storey drift values in rad)\n",
    "damage_thresholds    =  [0.00150, 0.00545, 0.00952, 0.0135] \n",
    "\n",
    "# The lower limit to be applied for censoring edp values (below 0.1 the minimum threshold for slight damage is considered a negligible case)\n",
    "lower_limit = 0.1*damage_thresholds[0]\n",
    "\n",
    "# The upper limit to be applied for consoring edp values (above 1.5 the maximum threshold is considered a collapse case) \n",
    "censored_limit = 1.5*damage_thresholds[-1]   \n",
    "\n",
    "# Define consequence model to relate structural damage to a decision variable (i.e., expected loss ratio) \n",
    "consequence_model = [0.05, 0.20, 0.60, 1.00] # damage-to-loss ratios"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "229fa396-0d68-4f92-9f10-ec1cce738070",
   "metadata": {},
   "source": [
    "## Define Directories and Import Analysis Data ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79a6ea8f-981b-4d52-8952-d3dfc1f7448a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the directory of the ground-motion records and import the intensity measure pickle file containing all IMs (processed from example 1)\n",
    "gm_directory  = f'{vmtk}/demos/in/records'            \n",
    "ims           = import_from_pkl(f'{gm_directory}/imls_esrm20.pkl')     \n",
    "\n",
    "# Define the main output directory and import the analysis output from a pickle file using the \"import_from_pkl\" function from \"utilities\"\n",
    "nltha_directory = f'{vmtk}/demos/out/nltha' \n",
    "ansys_dict      = import_from_pkl(f'{nltha_directory}/ansys_out.pkl') # processed from example 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15cc76f7-4b3e-44b9-bbed-0e6b7c34dd4a",
   "metadata": {},
   "source": [
    "## Post-Processing: Fragility Analysis on Cloud Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93315609-c1ce-445a-8232-476470c5f6b7",
   "metadata": {},
   "source": [
    "#### The cloud analysis module (do_cloud_analysis) of the \"postprocessor\" class requires five mandatory arguments and one optional:\n",
    "1. The intensity measure levels of the ground-motion records (imls)\n",
    "2. The engineering demand parameters from the analysis (edps)\n",
    "3. The demand-based damage thresholds (damage_thresholds)\n",
    "4. The lower edp censoring limit (lower_limit)\n",
    "5. The upper edp censoring limit (censored_limit)\n",
    "6. OPTIONAL: The modelling uncertainty (sigma_build2build), default is set 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e95bb1af-551d-4db1-b3d1-c3b4e0ffedb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop over the intensity measure types and perform cloud regression to fit the probabilistic seismic demand-capacity model\n",
    "for _, current_imt in enumerate(IMTs):\n",
    "    \n",
    "    # Import the current intensity measure type\n",
    "    imls = ims[f'{current_imt}']                   \n",
    "\n",
    "    # Import the engineering demand parameters (i.e., mpsd) from the analysis dictionary (processed from example 2)\n",
    "    edps = ansys_dict['mdof_max_peak_drift_list']  \n",
    "    \n",
    "    # Process cloud analysis results using the \"do_cloud_analysis\" function called from \"postprocessor\" \n",
    "    # The output will be automatically stored in a dictionary\n",
    "    cloud_dict = pp.do_cloud_analysis(imls,\n",
    "                                      edps,\n",
    "                                      damage_thresholds,\n",
    "                                      lower_limit,\n",
    "                                      censored_limit) \n",
    "    \n",
    "    ## Create a figures directory for each building class\n",
    "    figures_directory = f'{vmtk}/example/out/nltha/figures'    \n",
    "    if not os.path.exists(f'{figures_directory}'):\n",
    "        os.makedirs(f'{figures_directory}')\n",
    "    \n",
    "    ## Visualise the cloud analysis results\n",
    "    pl.plot_cloud_analysis(cloud_dict, \n",
    "                          figures_directory, \n",
    "                          plot_label = f'cloud_analysis_{current_imt}',\n",
    "                          xlabel = f'{current_imt} [g]', \n",
    "                          ylabel = r'Maximum Peak Storey Drift, $\\theta_{max}$ [%]') # The y-axis values of drift are converted to % automatically by the plotter\n",
    "\n",
    "    ## Visualise the fragility functions\n",
    "    pl.plot_fragility_analysis(cloud_dict,\n",
    "                               figures_directory,\n",
    "                               plot_label = f'fragility_{current_imt}',\n",
    "                               xlabel = f'{current_imt}')\n",
    "\n",
    "    ## Visualise the seismic demands\n",
    "    pl.plot_demand_profiles(ansys_dict['mdof_peak_drift_list'], \n",
    "                            ansys_dict['mdof_peak_accel_list'], \n",
    "                            ansys_dict['control_nodes'], \n",
    "                            figures_directory,\n",
    "                            plot_label=\"seismic_demand_profiles\") # The y-axis values of drift and acceleration are converted to % and g automatically by the plotter\n",
    "        \n",
    "    ## Visualise the entire set of results using subplots\n",
    "    pl.plot_ansys_results(cloud_dict,\n",
    "                          ansys_dict['mdof_peak_drift_list'],\n",
    "                          ansys_dict['mdof_peak_accel_list'],\n",
    "                          ansys_dict['control_nodes'],\n",
    "                          figures_directory,\n",
    "                          plot_label = f'analysis_output_{current_imt}',\n",
    "                          cloud_xlabel = f'{current_imt}',\n",
    "                          cloud_ylabel = r'Maximum Peak Storey Drift, $\\theta_{max}$ [%]')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40313e23-e64a-4bf7-a8bb-2662a4c93c95",
   "metadata": {},
   "source": [
    "## Post-Processing: Vulnerability Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e22987-92ab-4234-8dc5-82c744cd8bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To derive the vulnerability, the consequence model needs to convolved with the fragility functions. \n",
    "# To do so, we can use the \"get_vulnerability_function\" method from the \"postprocessor\" class.\n",
    "structural_loss = pp.get_vulnerability_function(cloud_dict['poes'],\n",
    "                                                consequence_model)\n",
    "\n",
    "# Plot the vulnerability function to visualise\n",
    "plt.plot(cloud_dict['intensities'], structural_loss, label = 'Structural Loss', color = COLOR, lw=LINEWIDTH_1)\n",
    "plt.xlabel('Spectral Acceleration, Sa(T=1.0s) [g]', fontsize= FONTSIZE_1) # This is because the latest cloud_dict variable is associated with SA(1.0s)\n",
    "plt.ylabel('Expected Loss Ratio', fontsize = FONTSIZE_1)\n",
    "plt.legend(loc = 'lower right')\n",
    "plt.grid(visible=True, which='major')\n",
    "plt.grid(visible=True, which='minor')\n",
    "plt.xlim([0.00, 5.00])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39c357a7-e6b1-43b3-a58b-f9cee926cdc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "end = time.time()\n",
    "print('Elapsed Time:', (end - start)/60, 'minutes')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
