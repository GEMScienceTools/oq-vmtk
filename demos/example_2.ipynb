{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "64c5fb72-db28-4180-86a8-07ca0e023679",
   "metadata": {},
   "source": [
    "# Example 2: Cloud Analysis on MDOF System\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This Jupyter Notebook provides a structured workflow for performing a nonlinear time-history analyses (NLTHA), namely cloud analysis on multi-degree-of-freedom (MDOF) structural models using natural unscaled ground-motion records. By combining functions for MDOF calibration, modeling and dynamic analysis, the notebook enables the setup, execution, and post-processing of structural responses under earthquake loading.\n",
    "\n",
    "The main goals of this notebook:\n",
    "\n",
    "1. **Calibrate MDOF models based on single-degree-of-freedom (SDOF) oscillator capacity**: Calibrate storey-based force-deformation relationships using SDOF capacity curve definition (spectral displacement-spectral acceleration) based on the methodology of Lu et al. (2020) and other modifications to account for distinct response typologies (i.e., bilinear, trilinear and quadrilinear backbone definitions)\n",
    "\n",
    "2. **Compile and construct MDOF Models in OpenSees**: Define and assemble MDOF models by specifying essential structural properties, including:\n",
    "   - Mass, heights, fundamental period, etc.\n",
    "   - Nonlinear response characteristics at each degree of freedom\n",
    "\n",
    "3. **Run Nonlinear Time-History Analysis (NLTHA) in OpenSees**: Simulate the dynamic response of MDOF structures under time-dependent inputs, such as ground motion records, to realistically assess structural behavior and response metrics (e.g., peak storey drifts, peak floor accelerations) under loading conditions and extract critical response metrics and model information.\n",
    "\n",
    "4. **Fragility Analysis**: Postprocess cloud analysis results to fit a probabilistic seismic demand model (PSDM) to ultimately estimate the median seismic intensities and total dispersion associated with arbitrary demand-based thresholds (i.e., maximum interstorey drift-based) and construct the fragility functions relating damage state exceedance probabilities with increasing levels of ground-shaking.\n",
    "\n",
    "5. **Vulnerability Analysis**: Integrate fragility functions with consequence models (i.e., damage-to-loss models) to determine the continuous relationship between a decision variable and increasing levels of ground-shaking.\n",
    "\n",
    "\n",
    "The notebook provides a step-by-step guide, covering each phase from MDOF model calibration, setup to input parameter configuration, analysis execution, and detailed results extraction and postprocessing. Users should have some familiarity with python scripts, structural dynamics, computational modeling, and performance-based earthquake engineering to fully benefit from this material.\n",
    "\n",
    "---\n",
    "\n",
    "## References\n",
    "\n",
    "[1] Lu X, McKenna F, Cheng Q, Xu Z, Zeng X, Mahin SA. An open-source framework for regional earthquake loss estimation using the city-scale nonlinear time history analysis. Earthquake Spectra. 2020;36(2):806-831. doi:10.1177/8755293019891724"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59a3487b-9409-420e-8623-b1a521a3a0c2",
   "metadata": {},
   "source": [
    "## Initialize Libraries ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f93e9dd2-9ed0-430f-9c12-2be3f3908292",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import shutil\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Import the classes necessary for structural analysis\n",
    "from openquake.vmtk.units         import units              # oq-vtmk units class\n",
    "from openquake.vmtk.calibration   import calibrate_model    # oq-vmtk sdof-to-mdof calibration class\n",
    "from openquake.vmtk.modeller      import modeller           # oq-vmtk numerical modelling class\n",
    "from openquake.vmtk.postprocessor import postprocessor      # oq-vtmk postprocessing class\n",
    "from openquake.vmtk.plotter       import plotter            # oq-vmtk plotting class\n",
    "from openquake.vmtk.utilities     import sorted_alphanumeric, import_from_pkl, export_to_pkl # oq-vmtk utility class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1054683-d8b7-4110-8ec5-5a0a2d0da11a",
   "metadata": {},
   "source": [
    "## Define Directories ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e94d49ea-44b5-4bbe-98bd-f5e15abfa785",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the directory of the ground-motion records\n",
    "gm_directory  = './in/records'            \n",
    "\n",
    "# Define the main output directory\n",
    "nrha_directory = './out/nltha'  \n",
    "os.makedirs(nrha_directory, exist_ok=True)\n",
    "\n",
    "# Define directory for temporary analysis outputs: it is used to store temporary .txt files used as accelerations recorders\n",
    "temp_nrha_directory = os.path.join(nrha_directory,'temp')\n",
    "os.makedirs(temp_nrha_directory, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbc4928d-927a-470a-b474-6295fdc66078",
   "metadata": {},
   "source": [
    "## Import Analysis Data ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1a26a70-ebc6-4592-ad06-3dd7d1fe3ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the intensity measure dictionary (output from example 1)\n",
    "ims           = import_from_pkl(os.path.join(gm_directory, 'imls_esrm20.pkl'))  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88ae2a14-a644-46bf-b270-fdf67eedd8aa",
   "metadata": {},
   "source": [
    "## Required Input Parameters ##"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c555c9f7-761b-4792-8bae-4ce623f788a8",
   "metadata": {},
   "source": [
    "### Modelling Input Parameters ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b07b3ef-e388-4917-9526-8b6d14fedab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of storeys\n",
    "number_storeys = 2\n",
    "\n",
    "# Relative floor heights list\n",
    "floor_heights = [2.80, 2.80]\n",
    "\n",
    "# First-mode based participation factor\n",
    "gamma = 1.33\n",
    "\n",
    "# SDOF capacity (First row are Spectral Displacement [m] values - Second row are Spectral Acceleration [g] values)\n",
    "sdof_capacity = np.array([[0.00060789, 0.00486316, 0.02420000, 0.04353684], \n",
    "                          [0.10315200, 0.20630401, 0.12378241, 0.12502023]]).T\n",
    "# Frame flag\n",
    "isFrame = False\n",
    "\n",
    "# Soft-storey mechanism flag\n",
    "isSOS = False\n",
    "\n",
    "# Degradation flag \n",
    "mdof_degradation = True\n",
    "\n",
    "# Inherent damping \n",
    "mdof_damping = 0.05"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43a8cc66-1129-4d5f-99d6-063760f1326b",
   "metadata": {},
   "source": [
    "### Fragility and Vulnerability Input Parameters ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "887a6c83-d644-4b49-a1ab-8bbc9c561634",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intensity measures to use for postprocessing cloud analyses\n",
    "IMTs      = ['PGA', 'SA(0.3s)', 'SA(0.6s)', 'SA(1.0s)','AvgSA']\n",
    "\n",
    "# Damage thresholds (maximum peak storey drift values in rad)\n",
    "damage_thresholds    =  [0.00150, 0.00545, 0.00952, 0.0135] \n",
    "\n",
    "# The lower limit to be applied for censoring edp values (below 0.1 the minimum threshold for slight damage is considered a negligible case)\n",
    "lower_limit = 0.1*damage_thresholds[0]\n",
    "\n",
    "# The upper limit to be applied for consoring edp values (above 1.5 the maximum threshold is considered a collapse case) \n",
    "censored_limit = 1.5*damage_thresholds[-1]   \n",
    "\n",
    "# Define consequence model to relate structural damage to a decision variable (i.e., expected loss ratio) \n",
    "consequence_model = [0.05, 0.20, 0.60, 1.00] # damage-to-loss ratios"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32e4177c-af56-4aa5-af84-56fa5a3df348",
   "metadata": {},
   "source": [
    "## Cloud Analysis ##"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04aac30a-1595-44f4-9335-2bc5cc8f387e",
   "metadata": {},
   "source": [
    "Cloud Analysis is a method used in structural engineering to assess the fragility of structures under seismic events. It involves performing nonlinear dynamic analyses using a set of recorded ground motions without scaling them, and then applying simple linear regression in the logarithmic space of structural response versus seismic intensity. This approach allows for efficient estimation of structural fragility by considering the inherent variability in ground motions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f690ee9b-4fcf-4753-ae8c-6643b178265b",
   "metadata": {},
   "source": [
    "### Cloud Analysis - Part 1: Calibrate MDOF Model based on SDOF Capacity Definition ###\n",
    "\n",
    "#### The calibration function (calibrate_model) requires five input arguments:\n",
    "1. Number of storeys\n",
    "2. First-mode transformation factor (gamma)\n",
    "3. The capacity array of the single degree-of-freedom oscillator\n",
    "4. Boolean flag whether the lateral load-resisting system for the considered building class is moment-resisting frames or braced frames (or not)\n",
    "5. Boolean flag whether the building class expects a soft-storey mechanism to be activated (or not)\n",
    "#### The calibration function (calibrate_model) returns four output variables:\n",
    "1. The floor mass array to be assigned to the MDOF model generator (floor_masses)\n",
    "2. The storey deformation (in m) capacity to be assigned to the MDOF model generator (storey_disps)\n",
    "3. The acceleration capacity (in g) to be assigned to the MDOF model generator (storey_forces)\n",
    "4. The considered mode shape (mdof_phi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "994490a5-288f-430f-b139-2a6613cb5848",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calibrate the model using the Lu et al. (2020) method\n",
    "floor_masses, storey_disps, storey_forces, mdof_phi = calibrate_model(number_storeys, gamma, sdof_capacity, isFrame, isSOS)\n",
    "\n",
    "print('The mass of each floor (in tonnes):', floor_masses)\n",
    "print('The first-mode shape used for calibration:', mdof_phi)\n",
    "\n",
    "# Plot the capacities to visualise the outcome of the calibration\n",
    "for i in range(storey_disps.shape[0]):\n",
    "   plt.plot(np.concatenate(([0.0], storey_disps[i,:])), np.concatenate(([0.0], storey_forces[i,:]*9.81)), label = f'Storey #{i+1}')\n",
    "plt.plot(np.concatenate(([0.0], sdof_capacity[:,0])), np.concatenate(([0.0], sdof_capacity[:,1]*9.81)), label = 'SDOF Capacity')\n",
    "plt.xlabel('Storey Deformation [m]', fontsize= 16)\n",
    "plt.ylabel('Storey Shear [kN]', fontsize = 16)\n",
    "plt.legend(loc = 'lower right')\n",
    "plt.grid(visible=True, which='major')\n",
    "plt.grid(visible=True, which='minor')\n",
    "plt.xlim([0.00, 0.03])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14aa838b-dbc0-4a4d-825e-32977d40bf6b",
   "metadata": {},
   "source": [
    "### Cloud Analysis - Part 2: Setting Up, Running and Exporting Analysis ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d458a5-8eec-4aeb-b0bc-aaa5c37ea84d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise MDOF storage lists\n",
    "conv_index_list = []               # List for convergence indices\n",
    "peak_disp_list  = []               # List for peak floor displacement (returns all peak values along the building height)\n",
    "peak_drift_list = []               # List for peak storey drift (returns all peak values along the building height)\n",
    "peak_accel_list = []               # List for peak floor acceleration (returns all peak values along the building height)\n",
    "max_peak_drift_list = []           # List for maximum peak storey drift (returns the maximum value) \n",
    "max_peak_drift_dir_list = []       # List for maximum peak storey drift directions\n",
    "max_peak_drift_loc_list = []       # List for maximum peak storey drift locations\n",
    "max_peak_accel_list = []           # List for maximum peak floor acceleration (returns the maximum value)\n",
    "max_peak_accel_dir_list = []       # List for maximum peak floor acceleration directions \n",
    "max_peak_accel_loc_list = []       # List for maximum peak floor acceleration locations\n",
    "\n",
    "# Loop over ground-motion records, compile MDOF model and run NLTHA\n",
    "gmrs = sorted_alphanumeric(os.listdir(os.path.join(gm_directory,'acc')))                         # Sort the ground-motion records alphanumerically\n",
    "dts  = sorted_alphanumeric(os.listdir(os.path.join(gm_directory,'dts')))                         # Sort the ground-motion time-step files alphanumerically\n",
    "\n",
    "# Run the analysis\n",
    "for i in range(len(gmrs)):\n",
    "    ### Print post-processing iteration\n",
    "    print('================================================================')\n",
    "    print('============== Analysing: {:d} out of {:d} =================='.format(i+1, len(gmrs)))\n",
    "    print('================================================================')\n",
    "\n",
    "    ### Compile the MDOF model    \n",
    "    model = modeller(number_storeys,\n",
    "                     floor_heights,\n",
    "                     floor_masses,\n",
    "                     storey_disps,\n",
    "                     storey_forces*units.g,\n",
    "                     mdof_degradation)                                                                # Initialise the class (Build the model)\n",
    "    \n",
    "    model.compile_model()                                                                             # Compile the MDOF model\n",
    "    \n",
    "    if i==0:\n",
    "        model.plot_model()                                                                            # Visualise the model (only on first iteration)        \n",
    "    model.do_gravity_analysis()                                                                       # Do gravity analysis\n",
    "\n",
    "    if number_storeys == 1:\n",
    "        num_modes = 1\n",
    "    else:\n",
    "        num_modes = 3\n",
    "    T, phi = model.do_modal_analysis(num_modes = num_modes)                                           # Do modal analysis and get period of vibration (Essential step for running NLTHA)\n",
    "\n",
    "    ### Define ground motion objects\n",
    "    fnames = [os.path.join(gm_directory,'acc',f'{gmrs[i]}')]                                          # Ground-motion record names\n",
    "    fdts   =  os.path.join(gm_directory,'dts',f'{dts[i]}')                                            # Ground-motion time-step names \n",
    "    dt_gm = pd.read_csv(fdts, header=None)[pd.read_csv(fdts,header=None).columns[0]].loc[1]-\\\n",
    "            pd.read_csv(fdts, header=None)[pd.read_csv(fdts,header=None).columns[0]].loc[0]           # Ground-motion time-step\n",
    "    t_max = pd.read_csv(fdts)[pd.read_csv(fdts).columns[0]].iloc[-1]                                  # Ground-motion duration\n",
    "    \n",
    "    ### Define analysis params and do NLTHA\n",
    "    dt_ansys = dt_gm                                                            # Set the analysis time-step\n",
    "    sf = units.g                                                                # Set the scaling factor (if records are in g, a scaling factor of 9.81 m/s2 must be used to be consistent with opensees) \n",
    "    control_nodes, conv_index, peak_drift, peak_accel, max_peak_drift, max_peak_drift_dir, max_peak_drift_loc, max_peak_accel, max_peak_accel_dir, max_peak_accel_loc, peak_disp = model.do_nrha_analysis(fnames, \n",
    "                                                                                                                                                                                                          dt_gm, \n",
    "                                                                                                                                                                                                          sf, \n",
    "                                                                                                                                                                                                          t_max, \n",
    "                                                                                                                                                                                                          dt_ansys,\n",
    "                                                                                                                                                                                                          temp_nrha_directory,\n",
    "                                                                                                                                                                                                          pflag=False,\n",
    "                                                                                                                                                                                                          xi = mdof_damping)\n",
    "\n",
    "    ### Store the analysis\n",
    "    conv_index_list.append(conv_index)\n",
    "    peak_drift_list.append(peak_drift)\n",
    "    peak_accel_list.append(peak_accel)\n",
    "    peak_disp_list.append(peak_disp)\n",
    "    max_peak_drift_list.append(max_peak_drift)\n",
    "    max_peak_drift_dir_list.append(max_peak_drift_dir)\n",
    "    max_peak_drift_loc_list.append(max_peak_drift_loc)\n",
    "    max_peak_accel_list.append(max_peak_accel)\n",
    "    max_peak_accel_dir_list.append(max_peak_accel_dir)\n",
    "    max_peak_accel_loc_list.append(max_peak_accel_loc)\n",
    "\n",
    "# Remove the temporary directory\n",
    "shutil.rmtree(f'{temp_nrha_directory}')\n",
    "\n",
    "# Store the analysis results in a dictionary\n",
    "ansys_dict = {}\n",
    "labels = ['T','control_nodes', 'conv_index_list',\n",
    "          'peak_drift_list','peak_accel_list',\n",
    "          'max_peak_drift_list', 'max_peak_drift_dir_list', \n",
    "          'max_peak_drift_loc_list','max_peak_accel_list',\n",
    "          'max_peak_accel_dir_list','max_peak_accel_loc_list',\n",
    "          'peak_disp_list']\n",
    "\n",
    "for i, label in enumerate(labels):\n",
    "    ansys_dict[label] = vars()[f'{label}']\n",
    "# Export the analysis output variable to a pickle file using the \"export_to_pkl\" function from \"utilities\"\n",
    "export_to_pkl(os.path.join(nrha_directory,'ansys_out.pkl'), ansys_dict) \n",
    "\n",
    "print('ANALYSIS COMPLETED!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a8aa6d2-4961-4f7a-a10f-0016e95f4ab0",
   "metadata": {},
   "source": [
    "### Cloud Analysis - Part 3: Constructing Fragility Functions ### \n",
    "#### The cloud analysis module (do_cloud_analysis) of the \"postprocessor\" class requires five mandatory arguments and one optional:\n",
    "1. The intensity measure levels of the ground-motion records (imls)\n",
    "2. The engineering demand parameters from the analysis (edps)\n",
    "3. The demand-based damage thresholds (damage_thresholds)\n",
    "4. The lower edp censoring limit (lower_limit)\n",
    "5. The upper edp censoring limit (censored_limit)\n",
    "6. OPTIONAL: The modelling uncertainty (sigma_build2build), default is set 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5187fd2b-1195-439f-b5e1-3f16422f0bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise the postprocessor class\n",
    "pp = postprocessor()\n",
    "\n",
    "# Initialise the plotter class\n",
    "pl = plotter()\n",
    "\n",
    "# Loop over the intensity measure types and perform cloud regression to fit the probabilistic seismic demand-capacity model\n",
    "for _, current_imt in enumerate(IMTs):\n",
    "    \n",
    "    # Import the current intensity measure type\n",
    "    imls = ims[f'{current_imt}']                   \n",
    "\n",
    "    # Import the engineering demand parameters (i.e., mpsd) from the analysis dictionary (processed from example 2)\n",
    "    edps = ansys_dict['max_peak_drift_list']  \n",
    "    \n",
    "    # Process cloud analysis results using the \"do_cloud_analysis\" function called from \"postprocessor\" \n",
    "    # The output will be automatically stored in a dictionary\n",
    "    cloud_dict = pp.do_cloud_analysis(imls,\n",
    "                                      edps,\n",
    "                                      damage_thresholds,\n",
    "                                      lower_limit,\n",
    "                                      censored_limit) \n",
    "        \n",
    "    ## Visualise the cloud analysis results\n",
    "    pl.plot_cloud_analysis(cloud_dict, \n",
    "                          output_directory = None, \n",
    "                          plot_label = f'cloud_analysis_{current_imt}',\n",
    "                          xlabel = f'{current_imt} [g]', \n",
    "                          ylabel = r'Maximum Peak Storey Drift, $\\theta_{max}$ [%]') # The y-axis values of drift are converted to % automatically by the plotter\n",
    "\n",
    "    ## Visualise the fragility functions\n",
    "    pl.plot_fragility_analysis(cloud_dict,\n",
    "                               output_directory = None,\n",
    "                               plot_label = f'fragility_{current_imt}',\n",
    "                               xlabel = f'{current_imt}')\n",
    "\n",
    "    ## Visualise the seismic demands\n",
    "    pl.plot_demand_profiles(ansys_dict['peak_drift_list'], \n",
    "                            ansys_dict['peak_accel_list'], \n",
    "                            ansys_dict['control_nodes'], \n",
    "                            output_directory = None,\n",
    "                            plot_label=\"seismic_demand_profiles\") # The y-axis values of drift and acceleration are converted to % and g automatically by the plotter\n",
    "        \n",
    "    ## Visualise the entire set of results using subplots\n",
    "    pl.plot_ansys_results(cloud_dict,\n",
    "                          ansys_dict['peak_drift_list'],\n",
    "                          ansys_dict['peak_accel_list'],\n",
    "                          ansys_dict['control_nodes'],\n",
    "                          output_directory = None,\n",
    "                          plot_label = f'analysis_output_{current_imt}',\n",
    "                          cloud_xlabel = f'{current_imt}',\n",
    "                          cloud_ylabel = r'Maximum Peak Storey Drift, $\\theta_{max}$ [%]')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aca9c45f-5ac7-48e8-a64f-b71a85630536",
   "metadata": {},
   "source": [
    "### Cloud Analysis Part 4: Constructing Vulnerability Functions ###\n",
    "\n",
    "To derive the vulnerability, the consequence model needs to convolved with the fragility functions.  To do so, we can use the \"get_vulnerability_function\" method from the \"postprocessor\" class. Setting the uncertainty to True will additionally calculate the coefficient of variation to explicitly consider the uncertainty in the Loss|IM as per Silva et al. (2019)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58f477b9-07ca-4206-9c41-8fe50da04ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this example, since the latest iteration of the previous cell uses 'AvgSA' as the intensity measure,\n",
    "# then all variables stored inside the \"cloud_dict\" dictionary correspond to that same IM. Hence, \n",
    "# the vulnerability function derived here will represent the continuous relationship of the expected \n",
    "# structural loss ratio conditioned on increasing levels of ground-shaking expressed in terms of the \n",
    "# average spectral acceleration (in g)\n",
    "\n",
    "structural_vulnerability = pp.get_vulnerability_function(cloud_dict['fragility']['poes'],\n",
    "                                                         consequence_model,\n",
    "                                                         uncertainty=True)\n",
    "\n",
    "\n",
    "# Plot the structural vulnerability function\n",
    "pl.plot_vulnerability_analysis(structural_vulnerability['IML'],\n",
    "                               structural_vulnerability['Loss'],\n",
    "                               structural_vulnerability['COV'],\n",
    "                               'SA(1.0s)',\n",
    "                               'Structural Loss Ratio',\n",
    "                               output_directory = None,\n",
    "                               plot_label = 'Structural Vulnerability')\n",
    "\n",
    "\n",
    "# The output is a DataFrame with three keys: IMLs (i.e., intensity measure levels), Loss and COV\n",
    "print(structural_vulnerability)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c75464b2-2474-45d7-b2ed-da77d4ff9169",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
