{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c8198fad-9570-4a67-8357-cadab89a3310",
   "metadata": {},
   "source": [
    "# Storey Loss Functions Applications in Vulnerability/Loss Model Calculation\n",
    "\n",
    "## Overview\n",
    "\n",
    "This Jupyter Notebook outlines the workflow for applying storey loss functions (SLFs) to derive a building-class vulnerability model. Using pre-existing SLFs, the workflow focuses on quantifying expected losses at each storey and combining them into a system-level vulnerability representation.\n",
    "\n",
    "The main goals of this notebook:\n",
    "\n",
    "1. **Storey Loss Function Application**: For each storey and seismic demand level, calculate the expected storey losses by interpolating from the provided SLFs (derived from the \"StoreyLossFunctionGeneration\" demo)\n",
    "\n",
    "2. **Aggregate Total Building Loss**: Sum the interpolated storey losses to compute the total building loss at each Intensity Measure (IM) level.\n",
    "\n",
    "3. **Fit Loss vs. IM Model**: Fit a regression model to relate the total building loss to IM level.\n",
    "\n",
    "4. **Condition Non-Collapse Losses**: Adjust repair-related (non-collapse) losses by the system-level collapse fragility to produce the final vulnerability model for the building class.\n",
    "\n",
    "---\n",
    "\n",
    "## References\n",
    "\n",
    "[1] Papadopoulos AN, Vamvatsikos D, Kazantzi AK. Development and Application of FEMA P-58 Compatible Story Loss Functions. Earthquake Spectra. 2019;35(1):95-112. doi:10.1193/102417EQS222M\n",
    "  \n",
    "[2] O’Reilly, G. J., & Shahnazaryan, D. (2024). On the utility of story loss functions for regional seismic vulnerability modeling and risk assessment. Earthquake Spectra, 40(3), 1933–1955. https://doi.org/10.1177/87552930241245940"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4e429f2-3342-48ad-82c4-5a8d1e347ade",
   "metadata": {},
   "source": [
    "## Import Libraries ## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa0f9f2-68c4-4201-b9ef-5d90031e4477",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "from scipy.optimize import curve_fit\n",
    "from scipy.stats import lognorm, genpareto\n",
    "\n",
    "# Import the classes necessary for postprocessing and visualising storey loss functions\n",
    "from openquake.vmtk.plotter import plotter\n",
    "from openquake.vmtk.utilities import import_from_pkl \n",
    "from openquake.vmtk.postprocessor import postprocessor\n",
    "\n",
    "# Initialise the plotter class\n",
    "pl = plotter()\n",
    "\n",
    "# Initialise the postprocessor class\n",
    "pp = postprocessor()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ede6b29b-ffb3-45d2-90e7-95f6af179064",
   "metadata": {},
   "source": [
    "## Load Required Input Data ## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db7bdeb9-26e7-465a-a739-83a9e86eeecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The set of drift- and acceleration-sensitive inventory of nonstructural components are compatible \n",
    "# with the FEMA P-58 database and were obtained using the \"Normative Quantity Estimation Tool\"\n",
    "# (https://femap58.atcouncil.org/supporting-materials) assuming a residential occupancy class\n",
    "\n",
    "input_directory = './input'\n",
    "\n",
    "# Load the drift-sensitive nonstructural components storey loss functions\n",
    "slf_drift = import_from_pkl(os.path.join(input_directory, 'slf_drift.pkl'))\n",
    "\n",
    "# Load the acceleration-sensitive nonstructural components storey loss functions\n",
    "slf_accel = import_from_pkl(os.path.join(input_directory, 'slf_accel.pkl'))\n",
    "\n",
    "# Load the analysis dictionary containing response quantities (i.e., engineering demand parameters) such as peak storey drifts and peak floor \n",
    "# acceleration necessary for the subsequent analysis using SLFs.\n",
    "# This dictionary corresponds to the application of cloud analysis on a 3 storey MDOF model whose example application is available in the \n",
    "# \"CloudAnalysis\" demo folder.\n",
    "ansys_out = import_from_pkl(os.path.join(input_directory, 'ansys_out.pkl'))\n",
    "\n",
    "# Load the intensity measure levels dictionary file evaluated from the \"IntensityMeasureProcessing\" demo \n",
    "# which contains various IMs and their quantities. This input is useful for deriving the total nonstructural loss\n",
    "ims = import_from_pkl(os.path.join(input_directory, 'imls_esrm20.pkl'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9f1a333-27c1-4937-9723-0e78e0e28326",
   "metadata": {},
   "source": [
    "## Normalise the Imported Storey Loss Functions ## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7397a644-a6a1-4c5e-837c-185bc3cd36d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The SLFs we imported in the previous cell contains absolute values associated with \n",
    "# the costs of repairs with increasing levels of engineering demand parameters\n",
    "# For this application, we will normalise the costs by the expected total cost of repair\n",
    "# per performance groups. In this way, our SLFs will represent storey loss ratios instead \n",
    "# and will additionally the reflect the contribution of each performance group on the\n",
    "# overall storey losses\n",
    "\n",
    "# Get the maximum cost associated with repairing both drift- and acceleration-\n",
    "# sensitive components from SLFs\n",
    "max_value = max(slf_drift['PSD, NS']['slf'])+ max(slf_accel['PFA, NS']['slf'])\n",
    "\n",
    "# Normalise the drift-sensitive nonstructural SLF\n",
    "slf_drift_norm = [x/max_value for x in slf_drift['PSD, NS']['slf']]\n",
    "\n",
    "# Normalise the acceleration-sensitive nonstructural SLF\n",
    "slf_accel_norm = [x/max_value for x in slf_accel['PFA, NS']['slf']]\n",
    "\n",
    "# Plot the normalise SLFs vs their corresponding EDPs\n",
    "psd_range = slf_drift['PSD, NS']['edp_range'] # Peak storey drifts range used to generate the SLFs in \"StoreyLossFunctionGeneration\" demo\n",
    "pfa_range = slf_accel['PFA, NS']['edp_range'] # Peak floor accelerations range used to generate the SLFs in \"StoreyLossFunctionGeneration\" demo\n",
    "\n",
    "# Plot the results \n",
    "fig, axs = plt.subplots(1, 2, figsize=(12, 5))  # 1 row, 2 columns\n",
    "\n",
    "# --- Left plot: Drift-sensitive ---\n",
    "axs[0].plot(psd_range, slf_drift_norm)\n",
    "axs[0].set_xlabel('Peak Storey Drifts [rad]')\n",
    "axs[0].set_ylabel('Drift-Sensitive Storey Loss Ratio')\n",
    "axs[0].grid(True)\n",
    "axs[0].set_xlim([0, 0.1])\n",
    "axs[0].set_ylim([0, 1.0])\n",
    "axes[0].set_title(\"Drift-Sensitive Components\")\n",
    "\n",
    "# --- Right plot: Acceleration-sensitive ---\n",
    "axs[1].plot(pfa_range, slf_accel_norm)\n",
    "axs[1].set_xlabel('Peak Floor Accelerations [g]')\n",
    "axs[1].set_ylabel('Acceleration-Sensitive Storey Loss Ratio')\n",
    "axs[1].grid(True)\n",
    "axs[1].set_xlim([0, 5.0])\n",
    "axs[1].set_ylim([0, 1.0])\n",
    "axes[0].set_title(\"Acceleration-Sensitive Components\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05fee8e4-b982-47fc-ad29-5c3d836dbf3d",
   "metadata": {},
   "source": [
    "## Interpolate Storey Loss Ratio using Storey Loss Functions and Seismic Demands ## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62635f6c-354f-4b70-b8de-a244b38ed7a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let us visualise the seismic demands obtained from the analysis we \"supposedly\" carried out\n",
    "\n",
    "pl.plot_demand_profiles(ansys_out['peak_drift_list'], \n",
    "                        ansys_out['peak_accel_list'], \n",
    "                        ansys_out['control_nodes'], \n",
    "                        output_directory = None,\n",
    "                        plot_label=\"seismic_demand_profiles\") # The y-axis values of drift and acceleration are converted to % and g automatically by the plotter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7a49034-5752-44e4-bf2f-a6b113bd1559",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each ground-motion record, seismic demand profiles expressing the distribution of peak storey drifts and peak floor accelerations\n",
    "# along the height of the idealised MDOF model were recorded. Therefore, for each ground-motion record, we will use the demand profiles \n",
    "# and interpolate via the SLFs, the associated storey loss\n",
    "\n",
    "# Number of ground motion records used\n",
    "n_gmrs = len(ansys_out['peak_drift_list']) \n",
    "\n",
    "# Number of storeys\n",
    "number_storeys = len(ansys_out['control_nodes'])-1\n",
    "\n",
    "# Number of floors \n",
    "number_floors = number_storeys + 1 \n",
    "\n",
    "# Loss weights: # These weights assume that drift- and acceleration-sensitive components are uniformly distributed\n",
    "# along the height of the building. Therefore, their contribution to the total losses is the dot product of the storey loss\n",
    "# and the weights\n",
    "psd_weights = [1/number_storeys]*number_storeys \n",
    "\n",
    "pfa_weights = [1/number_floors]*number_floors\n",
    "\n",
    "# Create dictionary to store results for each GM record\n",
    "results = {\"drift_total_loss\": [],\n",
    "           \"accel_total_loss\": [],\n",
    "           \"total_loss\": [],\n",
    "           \"IM\": []}\n",
    "\n",
    "# Loop over ground motion records\n",
    "for i in range(n_gmrs):\n",
    "\n",
    "    ## Process losses from drift-sensitive components\n",
    "    \n",
    "    # Step 1: Get the drift profile at ground motion record \"i\"\n",
    "    current_drifts = ansys_out['peak_drift_list'][i][:,0] # The indexing corresponds to fetching the peak storey drifts of ground motion \"i\" in the X-direction\n",
    "\n",
    "    # Create list to append values\n",
    "    drift_storey_losses = []\n",
    "    \n",
    "    # Step 2: Loop over the number of storeys\n",
    "    for j in range(number_storeys):\n",
    "\n",
    "        # Step 2.1: Interpolate the loss corresponding to the peak drift value at each storey\n",
    "        drift_storey_loss = np.interp(current_drifts[j], psd_range, slf_drift_norm)\n",
    "        drift_storey_losses.append(drift_storey_loss)\n",
    "    \n",
    "    # Step 3: Get the total losses along the height of the building from drift-sensitive components\n",
    "    drift_storey_losses = np.array(drift_storey_losses)\n",
    "    drift_total_loss    = np.dot(psd_weights, drift_storey_losses)\n",
    "\n",
    "    ## Process losses from acceleration-sensitive components\n",
    "    \n",
    "    # Step 1: Get the drift profile at ground motion record \"i\"\n",
    "    current_accels = ansys_out['peak_accel_list'][i][:,0] # The indexing corresponds to fetching the peak floor accelerations of ground motion \"i\" in the X-direction\n",
    "\n",
    "    # Create list to append values\n",
    "    accel_floor_losses = []\n",
    "    \n",
    "    # Step 2: Loop over the number of floor\n",
    "    for j in range(number_floors):\n",
    "\n",
    "        # Step 2.1: Interpolate the loss corresponding to the peak acceleration value at each floor\n",
    "        accel_floor_loss = np.interp(current_accels[j]/9.81, pfa_range, slf_accel_norm) # The floor accelerations should be converted to g\n",
    "        accel_floor_losses.append(accel_floor_loss)\n",
    "    \n",
    "    # Step 3: Get the total losses along the height of the building from acceleration-sensitive components\n",
    "    accel_floor_losses = np.array(accel_floor_losses)\n",
    "    accel_total_loss    = np.dot(pfa_weights, accel_floor_losses)\n",
    "\n",
    "    # Step 4: Save results in dictionary\n",
    "    results[\"drift_total_loss\"].append(drift_total_loss)\n",
    "    results[\"accel_total_loss\"].append(accel_total_loss)\n",
    "    results[\"total_loss\"].append(drift_total_loss + accel_total_loss)\n",
    "    results[\"IM\"].append(ims['PGA'][i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ff9f8f3-b19a-4923-999c-bd8ddb0b002f",
   "metadata": {},
   "source": [
    "## Visualise the Total Losses of Drift- and Acceleration-Sensitive Nonstructural Components ## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b1db183-1a66-465a-a72c-77a3b370997a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create subplots\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))  # 1 row, 2 columns\n",
    "\n",
    "# --- Drift-sensitive plot ---\n",
    "axes[0].scatter(results[\"IM\"], results[\"drift_total_loss\"], color=\"tab:blue\", alpha=0.7)\n",
    "axes[0].set_xlabel(\"IM (PGA)\")\n",
    "axes[0].set_ylabel(\"Drift-Sensitive Total Loss\")\n",
    "axes[0].set_title(\"Drift-Sensitive Components\")\n",
    "\n",
    "# --- Acceleration-sensitive plot ---\n",
    "axes[1].scatter(results[\"IM\"], results[\"accel_total_loss\"], color=\"tab:orange\", alpha=0.7)\n",
    "axes[1].set_xlabel(\"IM (PGA)\")\n",
    "axes[1].set_ylabel(\"Acceleration-Sensitive Total Loss\")\n",
    "axes[1].set_title(\"Acceleration-Sensitive Components\")\n",
    "\n",
    "# Improve layout\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d04f5bf2-df72-47bf-8f2c-2cf6cc7cd05f",
   "metadata": {},
   "source": [
    "## Visualise the Combined Total Losses of Nonstructural Components and Fit Regression Model for Nonstructural Vulnerability ## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e87cda-cf65-4f03-9c6e-4cf03a108a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, we fit a model to the total loss vs IM. We will explore various\n",
    "# regression models suitable for values whose range vary between 0 and 1. \n",
    "# In this application, we refer to the lognormal CDF, generalised pareto\n",
    "# distribution (GPD), the weibull function and the Papadopoulos [1] \n",
    "# nonlinear model. Eventually, we will select the best regression model \n",
    "# based on the goodness of fit parameter, SSE\n",
    "\n",
    "# --- Load data ---\n",
    "IM = np.array(results[\"IM\"])\n",
    "total_loss = np.array(results[\"total_loss\"])\n",
    "\n",
    "# Sort data by IM for fitting stability\n",
    "sorted_idx = np.argsort(IM)\n",
    "IM_sorted = IM[sorted_idx]\n",
    "total_loss_sorted = total_loss[sorted_idx]\n",
    "\n",
    "# Define regression models\n",
    "models = {\n",
    "    \"Lognormal\": {\n",
    "        \"func\": lambda x, mean, sigma: lognorm.cdf(x, s=sigma, scale=np.exp(mean)),\n",
    "        \"p0\": [0.0, 1.0]\n",
    "    },\n",
    "    \"GPD\": {\n",
    "        \"func\": lambda x, c, loc, scale: genpareto.cdf(x, c, loc=loc, scale=scale),\n",
    "        \"p0\": [0.1, 0.0, 1.0]\n",
    "    },\n",
    "    \"Weibull\": {\n",
    "        \"func\": lambda x, a, b, c: a * (1 - np.exp(-((x / b) ** c))),\n",
    "        \"p0\": [1.0, 1.0, 1.0]\n",
    "    },\n",
    "    \"Papadopoulos\": {\n",
    "        \"func\": lambda x, a, b, c, d, e: e * (x ** a) / (b ** a + x ** a) + (1 - e) * (x ** c) / (d ** c + x ** c),\n",
    "        \"p0\": [1.0, 1.0, 1.0, 1.0, 0.5]\n",
    "    }\n",
    "}\n",
    "\n",
    "# Fit models \n",
    "sse_dict = {}\n",
    "fitted_params = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    try:\n",
    "        popt, _ = curve_fit(model[\"func\"], IM_sorted, total_loss_sorted, p0=model[\"p0\"], maxfev=10000)\n",
    "        fitted_params[name] = popt\n",
    "        fitted_curve = model[\"func\"](IM_sorted, *popt)\n",
    "        sse_dict[name] = np.sum((total_loss_sorted - fitted_curve) ** 2)\n",
    "    except RuntimeError:\n",
    "        print(f\"Fit did not converge for {name}\")\n",
    "        fitted_params[name] = None\n",
    "        sse_dict[name] = np.inf\n",
    "\n",
    "# Rank models by SSE\n",
    "ranked_models = sorted(sse_dict.items(), key=lambda x: x[1])\n",
    "best_model = ranked_models[0][0]\n",
    "\n",
    "# Create smooth IM grid for plotting\n",
    "intensities = np.geomspace(0.05, 5, 50)\n",
    "\n",
    "# Plot the scatter and the fitted regression models\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.scatter(IM_sorted, total_loss_sorted, color='black', label='Data', alpha=0.7)\n",
    "\n",
    "colors = {\"Lognormal\":\"tab:blue\", \"GPD\":\"tab:green\", \"Weibull\":\"tab:orange\", \"Papadopoulos\":\"tab:red\"}\n",
    "\n",
    "for name, model in models.items():\n",
    "    if fitted_params[name] is not None:\n",
    "        curve = model[\"func\"](IM_range, *fitted_params[name])\n",
    "        label = f\"{name} (best)\" if name == best_model else name\n",
    "        plt.plot(intensities, curve, color=colors[name], linewidth=2, label=label)\n",
    "\n",
    "plt.xlabel(\"IM (PGA)\")\n",
    "plt.ylabel(\"Nonstructural Components Total Loss\")\n",
    "plt.title(\"Regression Fit of Models to IM vs Total Loss\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.xlim([0, 5])\n",
    "plt.ylim([0, 1])\n",
    "plt.show()\n",
    "\n",
    "# Print table with short summary of goodness-of-fit \n",
    "table = pd.DataFrame(ranked_models, columns=[\"Model\", \"SSE\"])\n",
    "table[\"Rank\"] = np.arange(1, len(table)+1)\n",
    "print(table)\n",
    "\n",
    "# NOTE: The nonstructural vulnerability derived above represents the total loss ratio \n",
    "# associated with nonstructural components conditioned on non-collapse of the structure.\n",
    "# However, in vulnerability applications, it is necessary to condition the non-collapse \n",
    "# loss on the collapse fragility of the structure (i.e., due consideration of the probability \n",
    "# of collapse of the structure)\n",
    "\n",
    "# Extract the non-conditioned losses associated with the best-fit model\n",
    "best_curve_func = models[best_model][\"func\"]\n",
    "best_curve_params = fitted_params[best_model]\n",
    "\n",
    "nonconditioned_loss = best_curve_func(IM_range, *best_curve_params)\n",
    "\n",
    "# Now `nonconditioned_loss` contains the fitted losses on the smooth IM_range\n",
    "print(\"Best-fitting model:\", best_model)\n",
    "print(\"Nonconditioned losses (sampled on IM_range):\")\n",
    "print(nonconditioned_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d1cc312-40eb-485a-a326-039e53caac6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us consider that this case study structure has a median seismic intensity and \n",
    "# dispersion associated with complete damage of 2.0g of PGA and 0.3, respectively \n",
    "\n",
    "median_collapse = 2.0\n",
    "dispersion_collapse = 0.3\n",
    "\n",
    "# Let us now create the \"collapse\" fragility using OQ-VMTK's postprocessor module\n",
    "collapse_fragility = pp.calculate_lognormal_fragility(median_collapse,\n",
    "                                                      dispersion_collapse,\n",
    "                                                      intensities=intensities)\n",
    "\n",
    "# Plot the collapse fragility\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.plot(intensities, collapse_fragility, color='black', alpha=0.7)\n",
    "plt.xlabel('IM (PGA)')\n",
    "plt.ylabel('Collapse Fragility')\n",
    "plt.xlim([0,5])\n",
    "plt.ylim([0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24f206e5-4254-4993-96c1-a2c5d7aa85f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, we condition the non-collapse (NC) nonstructural losses on collapse (C) via the following equation\n",
    "# Loss = E[L,NC|IM]*(1-P(C|IM)) + P(C|IM)*1.00 where 1.0 corresponds to attaining a loss ratio of 1 (total replacement)\n",
    "\n",
    "conditioned_loss = nonconditioned_loss*(1-collapse_fragility) + collapse_fragility*1.00"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8c626b3-81ca-4ac3-8f31-0d8a0995b82a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise the non-collapse and conditioned loss model\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.plot(intensities, nonconditioned_loss, color='blue', label = 'Non-Conditioned Loss')\n",
    "plt.plot(intensities, conditioned_loss,    color='red',  label = 'Conditioned Loss')\n",
    "plt.xlabel('IM (PGA)')\n",
    "plt.ylabel('Nonstructural Components Total Loss')\n",
    "plt.legend()\n",
    "plt.grid('on', which = 'both')\n",
    "plt.xlim([0,5])\n",
    "plt.ylim([0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9daec457-3bd1-492d-94a6-641afab0a67e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
