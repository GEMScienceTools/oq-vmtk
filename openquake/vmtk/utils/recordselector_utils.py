### Import librariesimport osimport reimport sysimport statimport errnoimport shutilimport zipfileimport difflibimport requestsimport numpy as npfrom time import timefrom copy import deepcopyfrom datetime import datetimefrom scipy.stats import norm, qmcfrom openquake.baselib.node import Nodefrom openquake.hazardlib import nrmldef parse_sa_logic_tree_to_avgsa(input_logic_tree_file, output_logic_tree_file, avgsa_periods, correlation_model):    """    Details    -------    Parses the ordinary SA ground motion logic tree to an AvgSA equivalent    Parameters    ----------    input_logic_tree_file : str        Input GMPE logic tree for SA, e.g. 'gmmLT.xml'    output_logic_tree_file : str        The output GMPE LT file, e.g. 'gmmLT_AvgSA.xml'    avgsa_periods : list        Periods used for AvgSA calculation        e.g. periods = [0.4,0.5,0.6,0.7,0.8]    correlation_model: str        String for one of the supported correlation models (e.g. 'akkar', 'baker_jayaram')    Returns    -------    None.    """    def replace_text_str(input_str):        """        Details        -------        Replaces the text string of an uncertainty model with an alternative        formulation in terms of AvgSa        Parameters        ----------        input_str : str            Input string (return carriage delimited) describing entire uncertainty model        Returns        -------        None.        """        search_output = re.search(r'\[(.*?)\]', input_str)        if search_output:            input_gmpe = search_output.group(1)        else:            input_gmpe = input_str.strip()        # Setup initial arguments for GenericGmpeAvgSa        initial_set = ["[GenericGmpeAvgSA]",                        f"gmpe_name = \"{input_gmpe}\"",                        f"avg_periods = {avgsa_periods}",                        f"corr_func = \"{correlation_model}\""]        if not search_output:            # No additional arguments passed to GMPE, just return the string as is            return "\n".join(initial_set)        for isegment in input_str.split("\n"):            segment = isegment.strip()            if not segment:                # Empty string                continue            if input_gmpe in segment:                new_gmpe = segment.replace(input_gmpe, "GenericGmpeAvgSA")                if not new_gmpe in initial_set:                    initial_set.append(new_gmpe)            else:                initial_set.append(segment)        return "\n".join(initial_set)    [input_lt] = nrml.read(input_logic_tree_file)    output_lt = []    for blev in input_lt:        if blev.tag.endswith("logicTreeBranchingLevel"):            # Removes the branching level            bset = blev[0]        else:            # Has no branching level, only branch set            bset = deepcopy(blev)        bset_branches = []        for br in bset:            unc_model_str = br.uncertaintyModel.text            weight = float(br.uncertaintyWeight.text)            new_unc_model = replace_text_str(unc_model_str)            br_node = Node("logicTreeBranch", br.attrib, nodes=[Node("uncertaintyModel", text=new_unc_model), Node("uncertaintyWeight", text=str(weight))])            bset_branches.append(br_node)        output_bs = Node("logicTreeBranchSet", bset.attrib, nodes=bset_branches)        output_lt.append(output_bs)    output_lt = Node("logicTree", {"logicTreeID": input_lt["logicTreeID"] + "AvgSA"}, nodes=output_lt)    with open(output_logic_tree_file, "wb") as f:        nrml.write([output_lt], f, fmt="%s")    print("Written to %s" % output_logic_tree_file)def content_from_zip(paths, zip_name):    """    Details    -------    This function reads the contents of all selected records    from the zipfile in which the records are located    Parameters    ----------    paths : list        Containing file list which are going to be read from the zipfile.    zip_name : str        Path to the zip file where file lists defined in "paths" are located.    Returns    -------    contents : dictionary        Containing raw contents of the files which are read from the zipfile.    """    contents = {}    with zipfile.ZipFile(zip_name, 'r') as myzip:        for i in range(len(paths)):            with myzip.open(paths[i]) as myfile:                contents[i] = [x.decode('utf-8') for x in myfile.readlines()]    return contentsdef read_nga(in_filename=None, content=None, out_filename=None):    """    Details    -------    This function process acceleration history for NGA data file (.AT2 format).    Parameters    ----------    in_filename : str, optional        Location and name of the input file.        The default is None    content : str, optional        Raw content of the .AT2 file.        The default is None    out_filename : str, optional        location and name of the output file.        The default is None.    Notes    -----    At least one of the two variables must be defined: inFilename, content.    Returns    -------    dt : float        time interval of recorded points.    npts : int        number of points in ground motion record file.    desc : str        Description of the earthquake (e.g., name, year, etc).    t : numpy.ndarray (n x 1)        time array, same length with npts.    acc : numpy.ndarray (n x 1)        acceleration array, same length with time unit        usually in (g) unless stated as other.    """    try:        # Read the file content from inFilename        if content is None:            with open(in_filename, 'r') as inFileID:                content = inFileID.readlines()        # check the first line        temp = str(content[0]).split()        try:  # description is in the end            float(temp[0])  # do a test with str to float conversion, this will be ok if description is in the end.            # Description of the record            desc = content[-2]            # Number of points and time step of the record            row4Val = content[-4]            # Acceleration values            acc_data = content[:-4]        except ValueError:  # description is in the beginning            # Description of the record            desc = content[1]            # Number of points and time step of the record            row4Val = content[3]            # Acceleration values            acc_data = content[4:]        # Description of the record        desc = desc.replace('\r', '')        desc = desc.replace('\n', '')        # Number of points and time step of the record        if row4Val[0][0] == 'N':            val = row4Val.split()            if 'dt=' in row4Val:                dt_str = 'dt='            elif 'DT=' in row4Val:                dt_str = 'DT='            if 'npts=' in row4Val:                npts_str = 'npts='            elif 'NPTS=' in row4Val:                npts_str = 'NPTS='            if 'sec' in row4Val:                sec_str = 'sec'            elif 'SEC' in row4Val:                sec_str = 'SEC'            npts = int(val[(val.index(npts_str)) + 1].rstrip(','))            try:                dt = float(val[(val.index(dt_str)) + 1])            except ValueError:                dt = float(val[(val.index(dt_str)) + 1].replace(sec_str + ',', ''))        else:            val = row4Val.split()            npts = int(val[0])            dt = float(val[1])        # Acceleration values        acc = np.array([])        for line in acc_data:            acc = np.append(acc, np.array(line.split(), dtype=float))        dur = len(acc) * dt        t = np.arange(0, dur, dt)        if out_filename is not None:            np.savetxt(out_filename, acc, fmt='%1.4e')        return dt, npts, desc, t, acc    except BaseException as error:        print(f"Record file reader FAILED for {in_filename}: ", error)def read_esm(in_filename=None, content=None, out_filename=None):    """    Details    -------    This function process acceleration history for ESM data file.    Parameters    ----------    in_filename : str, optional        Location and name of the input file.        The default is None    content : str, optional        Raw content of the ESM record file.        The default is None    out_filename : str, optional        location and name of the output file.        The default is None.    Returns    -------    dt : float        time interval of recorded points.    npts : int        number of points in ground motion record file.    desc : str        Description of the earthquake (e.g., name, year, etc).    time : numpy.ndarray (n x 1)        time array, same length with npts.    acc : numpy.ndarray (n x 1)        acceleration array, same length with time unit        usually in (g) unless stated as other.    """    try:        # Read the file content from inFilename        if content is None:            with open(in_filename, 'r') as inFileID:                content = inFileID.readlines()        desc = content[:64]        dt = float(difflib.get_close_matches('SAMPLING_INTERVAL_S', content)[0].split()[1])        npts = len(content[64:])        acc_data = content[64:]        acc = np.asarray([float(data) for data in acc_data], dtype=float)        dur = len(acc) * dt        t = np.arange(0, dur, dt)        acc = acc / 980.655  # cm/s**2 to g        if out_filename is not None:            np.savetxt(out_filename, acc, fmt='%1.4e')        return dt, npts, desc, t, acc    except BaseException as error:        print(f"Record file reader FAILED for {in_filename}: ", error)# MISCELLANEOUS FUNCTIONS# ---------------------------------------------------------------------def get_esm_token(username, password):    """    Details    -------    This function retrieves ESM database token.    Notes    -------    Data must be obtained using any program supporting the HTTP-POST method, e.g. CURL.    see: https://esm-db.eu/esmws/generate-signed-message/1/query-options.html    Credentials must have been retrieved from https://esm-db.eu/#/home.    Parameters    ----------    username : str        Account username (e-mail),  e.g. 'example_username@email.com'.    password : str        Account password, e.g. 'example_password123456'.    Returns    -------    token_path : str        Path to token used to retrieve records from ESM_2018 database    """    if sys.platform.startswith('win'):        command = 'curl --ssl-no-revoke -X POST -F ' + '\"' + \                  'message={' + '\\\"' + 'user_email' + '\\\": ' + '\\\"' + username + '\\\", ' + \                  '\\\"' + 'user_password' + '\\\": ' + '\\\"' + password + '\\\"}' + \                  '\" ' + '\"https://esm-db.eu/esmws/generate-signed-message/1/query\" > token.txt'    else:        command = 'curl -X POST -F \'message={\"user_email\": \"' + \                  username + '\",\"user_password\": \"' + password + \                  '\"}\' \"https://esm-db.eu/esmws/generate-signed-message/1/query\" > token.txt'    os.system(command)    token_path = os.path.join(os.getcwd(), 'token.txt')    return token_pathdef make_dir(dir_path):    """    Details    -------    Makes a clean directory by deleting it if it exists.    Parameters    ----------    dir_path : str        name of directory to make.    None.    """    def handle_remove_read_only(func, path, exc):        excvalue = exc[1]        if func in (os.rmdir, os.remove) and excvalue.errno == errno.EACCES:            os.chmod(path, stat.S_IRWXU | stat.S_IRWXG | stat.S_IRWXO)  # 0777            func(path)        else:            raise Warning("Path is being used by at the moment.",                          "It cannot be recreated.")    if os.path.exists(dir_path):        shutil.rmtree(dir_path, ignore_errors=False, onerror=handle_remove_read_only)    os.makedirs(dir_path)def download_file_from_google_drive(file_id, destination):    """    Details    -------    Downloads the a file stored in google drive    Parameters    ----------    file_id : str        File id from google drive (last part of the shared link)    destination : str        The path where the downloaded file would be stored    Returns    -------    None.     """    URL = "https://docs.google.com/uc?export&confirm=download"    HEADERS = {    "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/39.0.2171.95 Safari/537.36"  # NOQA    }    CHUNK_SIZE = 32768    session = requests.Session()    response = session.get(URL, headers=HEADERS, params={'id': file_id}, stream=True)    token = None    for key, value in response.cookies.items():        if key.startswith('download_warning'):            token = value            break    if token:        params = {'id': file_id, 'confirm': token}        response = session.get(URL, headers=HEADERS, params=params, stream=True)    with open(destination, "wb") as f:        for chunk in response.iter_content(CHUNK_SIZE):            if chunk:  # filter out keep-alive new chunks                f.write(chunk)def run_time(start_time):    """    Details    -------    Prints the time passed between start_time and finish_time (now)    in hours, minutes, seconds. startTime is a global variable.    Parameters    ----------    start_time : int        The initial time obtained via time().    Returns    -------    None.    """    finish_time = time()    # Procedure to obtained elapsed time in Hr, Min, and Sec    time_seconds = finish_time - start_time    time_minutes = int(time_seconds / 60)    time_hours = int(time_seconds / 3600)    time_minutes = int(time_minutes - time_hours * 60)    time_seconds = time_seconds - time_minutes * 60 - time_hours * 3600    print(f"Run time: {time_hours:.0f} hours: {time_minutes:.0f} minutes: {time_seconds:.2f} seconds")def random_uniform(num_dimensions, num_samples, sampling_type):    """    Details    -------    Used to perform sampling based on Monte Carlo Simulation or Latin Hypercube Sampling    References    ----------    https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.qmc.LatinHypercube.html#scipy.stats.qmc.LatinHypercube    Parameters    ----------    num_dimensions : int        number of dimensions    num_samples : int        number of samples    sampling_type : str        type of sampling.        Monte Carlo Sampling: 'MCS'        Latin Hypercube Sampling: 'LHS'    Returns    -------    sample : numpy.ndarray (num_samples x num_dimensions)        Array which contains randomly generated numbers between 0 and 1    """    # Not really required, but will ensure different realizations each time    seed = int(datetime.today().strftime("%H%M%S"))    if sampling_type == 'MCS':        # Do Monte Carlo Sampling without any grid        np.random.seed(seed)        sample = np.random.uniform(size=[num_dimensions, num_samples]).T    elif sampling_type == 'LHS':        # A Latin hypercube sample generates n points in [0, 1)^d.        # Each univariate marginal distribution is stratified, placing exactly one point in each possible grid.        sampler = qmc.LatinHypercube(d=num_dimensions, seed=seed)        sample = sampler.random(n=num_samples)    return sampledef random_multivariate_normal(mu, cov, num_samples, sampling_option):    """    Details    -------    Used to generate multivariate correlated normal samples    References    ----------    Yang, T. Y., Moehle, J., Stojadinovic, B., & Der Kiureghian, A. (2009).    Seismic Performance Evaluation of Facilities: Methodology and Implementation.    In Journal of Structural Engineering (Vol. 135, Issue 10, pp. 1146â€“1154).    American Society of Civil Engineers (ASCE). https://doi.org/10.1061/(asce)0733-9445(2009)135:10(1146)    Parameters    ----------    mu : numpy.ndarray (1-D)        Mean value vector    cov : numpy.ndarray (2-D)        Covariance matrix    num_samples : int        number of samples    sampling_option : str        Monte Carlo Sampling: 'MCS'        Latin Hypercube Sampling: 'LHS'    Returns    -------    z : numpy.ndarray (num_samples x num_dimensions)        Array which contains randomly generated numbers between 0 and 1    """    num_dimensions = len(mu)    if mu.size == mu.shape[0]:        mu = mu.reshape(-1, 1)    my = mu @ np.ones([1, num_samples])    eigen_values, eigen_vectors = np.linalg.eigh(cov)    # The lower-triangular decomposition of the correlation matrix    ly = eigen_vectors    # Standard deviations    dy = np.diag(eigen_values ** 0.5)    # Generate uniformly distributed between 0 and 1    u = random_uniform(num_dimensions, num_samples, sampling_option)    # Compute standard random numbers    u = norm(loc=0, scale=1).ppf(u)    # Create realization matrix (Eqn. 4) - @ is the matrix multiplication    z = (ly @ dy @ u.T + my).T    return z